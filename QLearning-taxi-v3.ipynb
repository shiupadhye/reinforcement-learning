{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286e7fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f61f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\",render_mode=\"ansi\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ed5c4c8",
   "metadata": {},
   "source": [
    "Map:\n",
    "+---------+\n",
    "|R: | : :G|\n",
    "| : | : : |\n",
    "| : : : : |\n",
    "| | : | : |\n",
    "|Y| : |B: |\n",
    "+---------+\n",
    "\n",
    "Actions: N,S,E,W,pickup,dropoff (n=6)\n",
    "Observations: 500 discrete states (25 taxi locations * 5 passenger locations * 4 destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1bc143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of epsilon greedy action selection (multi-armed bandit)\n",
    "def epsilon_greedy_agent(epsilon,state,QTable):\n",
    "    x = bernoulli.rvs(1-epsilon, size=1)\n",
    "    if x == 1:\n",
    "        # exploit: select best performing action w/ probability 1 - epsilon\n",
    "        action = np.argmax(QTable[state])\n",
    "    else:\n",
    "        # explore: randomly select action for exploration w/ probability epsilon\n",
    "        action = env.action_space.sample()\n",
    "    return action\n",
    "\n",
    "# From Sutton and Barto (2018), p. 131\n",
    "def train_Qlearning(alpha,epsilon,gamma,episodes,maxSteps,verbose=False):\n",
    "    print(\"Begin training...\")\n",
    "    # initialize Q-table\n",
    "    QTable = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "    for ep in range(episodes):\n",
    "        if verbose and ep % 1000 == 0:\n",
    "            print(\"Episode %d\" % ep)\n",
    "        # initialize state\n",
    "        state = env.reset()[0]\n",
    "        isDone = False\n",
    "        for t in range(maxSteps):\n",
    "            # sample action from Q\n",
    "            action = epsilon_greedy_agent(epsilon,state,QTable)\n",
    "            # take action, observe reward and next state\n",
    "            next_state,reward,isDone,_,_ = env.step(action)\n",
    "            # update Q and state\n",
    "            QTable[state,action] += alpha * (reward + gamma * np.max(QTable[next_state]) - QTable[state,action])\n",
    "            state = next_state\n",
    "            # until state is terminal\n",
    "            if isDone == True:\n",
    "                break\n",
    "    return QTable    \n",
    "\n",
    "# Implementation of Watkin's Q-lambda (w/ eligibility traces)\n",
    "def train_QlearningLam(alpha,epsilon,gamma,lam,epsidoes,maxSteps,verbose=False):\n",
    "    print(\"Begin training...\")\n",
    "     # initialize Q-table\n",
    "    QTable = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "    # initialize eligibility traces\n",
    "    E = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "    for ep in range(episodes):\n",
    "        if verbose and ep % 1000 == 0:\n",
    "            print(\"Episode %d\" % ep)\n",
    "        # initialize state and action\n",
    "        state = env.reset()[0]\n",
    "        action = env.action_space.sample()\n",
    "        for t in range(maxSteps):\n",
    "            # take action a, observe reward and next state (s')\n",
    "            next_state,reward,isDone,_,_ = env.step(action)\n",
    "            # choose a' from s' using Q\n",
    "            next_action = epsilon_greedy_agent(epsilon,next_state,QTable)\n",
    "            # select a*\n",
    "            opt_action = np.argmax(QTable[next_state])\n",
    "            # compute TD error\n",
    "            TD = reward + gamma * QTable[next_state,next_action] - QTable[state,opt_action]\n",
    "            E[state,action] += 1\n",
    "            # for all states and actions\n",
    "            QTable[:] = QTable[:] + alpha * TD * E[:]\n",
    "            # build trace for greedy action\n",
    "            if next_action == opt_action:\n",
    "                E[:] = gamma * lam * E[:]\n",
    "            # zero out eligibility trace after non-greedy/exploratory action\n",
    "            else:\n",
    "                E[:] = 0\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            # continue until terminal state\n",
    "            if isDone:\n",
    "                break\n",
    "    return QTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8e8577",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b0e8ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "episodes = 50000\n",
    "maxSteps = 1000\n",
    "alpha = 0.1\n",
    "epsilon = 0.1\n",
    "gamma = 0.5\n",
    "lam = 0.1\n",
    "QTable = train_QlearningLam(alpha,epsilon,gamma,lam,episodes,maxSteps,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fabb45c",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7502b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = gym.make(\"Taxi-v3\",render_mode=\"human\")\n",
    "testEpisodes = 10\n",
    "epRewards = []\n",
    "epPenalities = []\n",
    "epTimesteps = []\n",
    "verbose = True\n",
    "for ep in range(testEpisodes):\n",
    "    if verbose:\n",
    "        print(\"Episode %d\" % (ep+1))\n",
    "    start_time = time.time()\n",
    "    state = test_env.reset()[0]\n",
    "    test_env.render()\n",
    "    rewards = []\n",
    "    isDone = False\n",
    "    rewards = 0\n",
    "    timesteps = 0\n",
    "    penalities = 0\n",
    "    while not isDone:\n",
    "        action = np.argmax(QTable[state,:])\n",
    "        next_state,reward,isDone,_,_ = test_env.step(action)\n",
    "        state = next_state\n",
    "        timesteps += 1\n",
    "        if reward == -10:\n",
    "            penalities += 1\n",
    "        rewards += reward\n",
    "    if verbose:\n",
    "        print(\"--- Completed in %s seconds ---\" % (time.time() - start_time))\n",
    "    epRewards.append(rewards)\n",
    "    epPenalities.append(penalities)\n",
    "    epTimesteps.append(timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fb4bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
